name: Refresh data and deploy Pages

on:
  schedule:
    - cron: "35 10 * * 1"   # Mondays 10:35 UTC (~6:35am ET)
  workflow_dispatch: {}

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout this repo
        uses: actions/checkout@v4

      - name: Checkout upstream reporter (sparse)
        uses: actions/checkout@v4
        with:
          repository: dhoconno/reporter
          path: _upstream
          lfs: true
          sparse-checkout: |
            data/processed
            README.md
          fetch-depth: 1

      - name: Prepare site data dir
        run: mkdir -p pages/data

      - name: Copy processed artifacts you need
        run: |
          # Copy everything under processed (as before)
          cp -R _upstream/data/processed/* pages/data/ || true
          # NEW: also pull any reporter files that look like award-amounts
          mkdir -p pages/data/reporter
          cp -v _upstream/data/processed/reporter/*amount* pages/data/reporter/ 2>/dev/null || true

      # NEW debug: list what (if anything) we grabbed that contains amounts
      - name: List award-amount candidates
        run: |
          echo "Looking for *amount* files under pages/data/reporter:"
          find pages/data/reporter -maxdepth 1 -type f -ls 2>/dev/null || true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyarrow zstandard

      - name: List copied files
        run: |
          echo "Tree under pages/data:"
          find pages/data -maxdepth 2 -type f -ls | sed 's/^/  /'

      - name: Preview awards columns
        run: |
          python - <<'PY'
          import pandas as pd
          p = "pages/data/reporter/nih_awards_all.csv.zst"
          df = pd.read_csv(p, compression="zstd", nrows=0)
          print("COLUMNS:", list(df.columns))
          PY

      - name: Build YTD 2024 vs 2025 aggregates
        run: |
          python scripts/build_ytd_2024_2025.py

      - name: Create data manifest (for sanity check)
        run: |
          python - <<'PY'
          import os, json
          root = "pages/data"
          files = []
          for dp, dn, fn in os.walk(root):
              for f in fn:
                  p = os.path.join(dp, f)
                  files.append({"path": os.path.relpath(p, root), "bytes": os.path.getsize(p)})
          os.makedirs(root, exist_ok=True)
          with open(os.path.join(root, "manifest.json"), "w") as fh:
              json.dump({"count": len(files), "files": files[:50]}, fh, indent=2)
          print(f"Wrote {root}/manifest.json with {len(files)} files")
          PY

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps (pandas + zstd)
        run: |
          pip install --upgrade pip
          pip install pandas pyarrow zstandard

      - name: Create data manifest (for sanity check)
        run: |
          python - <<'PY'
          import os, json
          root = "pages/data"
          files = []
          for dp, dn, fn in os.walk(root):
              for f in fn:
                  p = os.path.join(dp, f)
                  files.append({"path": os.path.relpath(p, root), "bytes": os.path.getsize(p)})
          os.makedirs(root, exist_ok=True)
          with open(os.path.join(root, "manifest.json"), "w") as fh:
              json.dump({"count": len(files), "files": files[:50]}, fh, indent=2)
          print(f"Wrote {root}/manifest.json with {len(files)} files")
          PY

      - name: Inspect upstream files & write samples
        run: |
          python - <<'PY'
          import pandas as pd, json, os
          base="pages/data"
          os.makedirs(f"{base}/samples", exist_ok=True)

          def save_head(path, name, compression=None, n=5):
              df = pd.read_csv(path, compression=compression, nrows=n)
              df.to_json(f"{base}/samples/{name}_head.json", orient="records", indent=2)
              with open(f"{base}/samples/{name}_columns.json","w") as f:
                  json.dump(list(df.columns), f, indent=2)

          save_head(f"{base}/reporter/nih_awards_all.csv.zst", "awards", compression="zstd")
          save_head(f"{base}/taggs/hhs_grants_terminated.csv", "taggs")
          save_head(f"{base}/federal_register/nih_fr_meetings_all.csv.zst", "federal_register", compression="zstd")
          print("Wrote sample head+columns JSONs")
          PY


      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: pages

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
